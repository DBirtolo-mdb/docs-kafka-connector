.. _kafka-sink-cdc:

==============================
Kafka Sink Change Data Capture
==============================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecols

Overview
--------

Change data capture (CDC) is an architecture that converts changes in
a source database into event streams. You can capture CDC events with the
MongoDB Kafka sink connector and perform corresponding insert, update, and
delete operations to a destination MongoDB cluster.

You can also handle CDC using the following event producers:

- :ref:`Debezium <cdc-debezium>`
- :ref:`Qlik Replicate <cdc-qlik>`

Change Data Capture Using the MongoDB Sink Connector
----------------------------------------------------

To configure your MongoDB Kafka sink connector to handle CDC events from
a Kafka topic, update your configuration to include the following:

.. code-block:: properties

   change.data.capture.handler=com.mongodb.kafka.connect.sink.cdc.mongodb.ChangeStreamHandler

The ``ChangeStreamHandler`` class instructs the sink connector to process
change events that are in the :manual:`change stream response document format </reference/change-events/#change-stream-output>`.
You can use a (TODO: FIX LINK) (MongoDB Kafka source connector </kafka-source>) to
configure the change stream data that you want to publish to specific topics.

Remember to specify the topic and the destination in the following
configuration properties:

- ``topics``
- ``connection.uri``
- ``database``
- ``collection``

For more information on the properties above, see our guide on
:doc:`Kafka Sink Connector Configuration Properties </kafka-sink-properties>`.


ChangeStreamHandler Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following sample JSON payload instantiates a new connector that uses
the ``ChangeStreamHandler`` with a specified CDC configuration when posted
to the `Kafka Connect REST endpoint <https://docs.confluent.io/current/connect/references/restapi.html>`__:

.. code-block:: json

   {
     "name": "mongo-sink-changestreamhandler-cdc",
     "config": {
       "connection.uri": "mongodb://<hostname>:27017/kafkaconnect?w=1&journal=true",
       "topics": "myreplset.kafkaconnect.mongosrc",
       "change.data.capture.handler": "com.mongodb.kafka.connect.sink.cdc.mongodb.ChangeStreamHandler",
       "connector.class": "com.mongodb.kafka.connect.sink.MongoSinkConnector",
       "key.converter": "io.confluent.connect.avro.AvroConverter",
       "key.converter.schema.registry.url": "http://<schema-registry-hostname>:8081",
       "value.converter": "io.confluent.connect.avro.AvroConverter",
       "value.converter.schema.registry.url": "http://<schema-registry-hostname>:8081",
       "collection": "mongosink"
     }
   }

.. include:: /includes/externalize-secrets.rst

.. _cdc-debezium:

Change Data Capture Using Debezium
----------------------------------

The MongoDB Kafka sink connector can also process event streams using
`Debezium <https://debezium.io/>`__ as an event producer for the following
source databases:

* `MongoDB <http://debezium.io/docs/connectors/mongodb/>`__
* `MySQL <http://debezium.io/docs/connectors/mysql/>`__
* `PostgreSQL <http://debezium.io/docs/connectors/postgresql/>`__

You can configure the sink connector to process data from a CDC stream
using one of the included handlers for Debezium or a custom handler that
extends the abstract class `CdcHandler
<https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/cdc/CdcHandler.java>`__.
The included handlers for Debezium can be found in the
`MongoDB Kafka connector repository here <https://github.com/mongodb/mongo-kafka/tree/master/src/main/java/com/mongodb/kafka/connect/sink/cdc/debezium>`__.

.. include:: /includes/create-cdc-handler.rst

.. _cdc-debezium-example:

Debezium Example
~~~~~~~~~~~~~~~~

The following sample JSON payload instantiates a new connector using
Debezium with a specified CDC configuration when posted to the
`Kafka Connect REST endpoint <https://docs.confluent.io/current/connect/references/restapi.html>`__:

.. code-block:: json

   {
     "name": "mongo-sink-debezium-cdc",
     "config": {
       "connection.uri": "mongodb://<mongodb-hostname>:27017/kafkaconnect?w=1&journal=true",
       "topics": "myreplset.kafkaconnect.mongosrc",
       "change.data.capture.handler": "com.mongodb.kafka.connect.sink.cdc.debezium.mongodb.MongoDbHandler",
       "connector.class": "com.mongodb.kafka.connect.sink.MongoSinkConnector",
       "key.converter": "io.confluent.connect.avro.AvroConverter",
       "key.converter.schema.registry.url": "http://<schema-registry-hostname>:8081",
       "value.converter": "io.confluent.connect.avro.AvroConverter",
       "value.converter.schema.registry.url": "http://<schema-registry-hostname>:8081",
       "collection": "mongosink"
     }
   }

.. include:: /includes/externalize-secrets.rst

.. _cdc-qlik:

Change Data Capture Using Qlik Replicate
----------------------------------------

The MongoDB Kafka sink connector can also process event streams using
`Qlik Replicate <https://www.qlik.com/us/products/qlik-replicate>`__ as an event
producer for several of data sources including:

- Oracle 
- Postgres
- Microsoft SQL Server

For a complete list of supported sources for Qlik Replicate CDC events, see the
`Qlik Replicate Source Endpoint Support Matrix <https://help.qlik.com/en-US/replicate/November2020-SR1/Content/Replicate/Main/Support%20Matrix/supported_source_endpoints.htm>`__.

You can configure the sink connector to process data from a CDC stream
using the included handler for Qlik Replicate or a custom handler that
extends the abstract class 
`CdcHandler <https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/cdc/CdcHandler.java>`__.
The included handler for Qlik Replicate can be found in the
`MongoDB Kafka connector repository here <https://github.com/mongodb/mongo-kafka/tree/master/src/main/java/com/mongodb/kafka/connect/sink/cdc/qlik>`__.

.. include:: /includes/create-cdc-handler.rst

.. _cdc-qlik-example:

Qlik Replicate Example
~~~~~~~~~~~~~~~~~~~~~~

The following sample JSON payload instantiates a new connector using
Qlik Replicate with a specified CDC configuration when posted to the
`Kafka Connect REST endpoint <https://docs.confluent.io/current/connect/references/restapi.html>`__:

.. code-block:: json

   {
     "name": "mongo-sink-qlik-replicate-cdc",
     "config": {
       "connector.class": "com.mongodb.kafka.connect.sink.MongoSinkConnector",
       "connection.uri": "mongodb://<mongodb-hostname>:27017/kafkaconnect?w=1&journal=true",
       "collection": "mongosink",
       "topics": "myreplset.kafkaconnect.rdbmssrc",
       "change.data.capture.handler": "com.mongodb.kafka.connect.sink.cdc.qlik.rdbms.RdbmsHandler",
       "value.converter": "org.apache.kafka.connect.json.JsonConverter"
     }
   }

.. include:: /includes/externalize-secrets.rst
